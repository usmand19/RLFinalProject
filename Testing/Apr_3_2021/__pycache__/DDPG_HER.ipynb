{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG(object):\n",
    "    def __init__(self, params):\n",
    "        \"\"\"Implementation of DDPG that is used in combination with Hindsight Experience Replay (HER).\n",
    "        Args:\n",
    "            input_dims (dict of ints): dimensions for the observation (o), the goal (g), and the\n",
    "                actions (u)\n",
    "            buffer_size (int): number of transitions that are stored in the replay buffer\n",
    "            polyak (float): coefficient for Polyak-averaging of the target network\n",
    "            batch_size (int): batch size for training\n",
    "            Q_lr (float): learning rate for the Q (critic) network\n",
    "            pi_lr (float): learning rate for the pi (actor) network\n",
    "            norm_eps (float): a small value used in the normalizer to avoid numerical instabilities\n",
    "            norm_clip (float): normalized inputs are clipped to be in [-norm_clip, norm_clip]\n",
    "            clip_obs (float): clip observations before normalization to be in [-clip_obs, clip_obs]\n",
    "            T (int): the time horizon for rollouts\n",
    "            rollout_batch_size (int): number of parallel rollouts per DDPG agent\n",
    "            clip_return (float): clip returns to be in [-clip_return, clip_return]\n",
    "            sample_transitions (function) function that samples from the replay buffer\n",
    "            gamma (float): gamma used for Q learning updates\n",
    "            reuse (boolean): whether or not the networks should be reused\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_dims = params['dims']\n",
    "        self.buffer_size = params['buffer_size']\n",
    "        self.polyak = params['polyak']\n",
    "        self.batch_size = params['batch_size']\n",
    "        self.Q_lr = params['lr']\n",
    "        self.pi_lr = params['lr']\n",
    "        self.norm_eps = params['norm_eps']\n",
    "        self.norm_clip = params['norm_clip']\n",
    "        self.clip_obs = params['clip_obs']\n",
    "        self.T = params['T']\n",
    "        self.rollout_batch_size = params['num_workers']\n",
    "        self.clip_return = params['clip_return']\n",
    "        self.sample_transitions = params['sample_her_transitions']\n",
    "        self.gamma = params['gamma']\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.replay_strategy = params['replay_strategy']\n",
    "        if self.replay_strategy == 'future':\n",
    "            self.use_goal = True\n",
    "        else:\n",
    "            self.use_goal = False\n",
    "\n",
    "        self.dimo = self.input_dims['o']\n",
    "        self.dimg = self.input_dims['g']\n",
    "        self.dimu = self.input_dims['u']\n",
    "\n",
    "        stage_shapes = OrderedDict()\n",
    "        for key in sorted(self.input_dims.keys()):\n",
    "            if key.startswith('info_'):\n",
    "                continue\n",
    "            stage_shapes[key] = (None, self.input_dims[key])\n",
    "        stage_shapes['o_2'] = stage_shapes['o']\n",
    "        stage_shapes['r'] = (None,)\n",
    "        self.stage_shapes = stage_shapes\n",
    "\n",
    "        self.create_network()\n",
    "\n",
    "        # Configure the replay buffer.\n",
    "        buffer_shapes = {key: (self.T-1 if key != 'o' else self.T, self.input_dims[key])\n",
    "                         for key, val in self.input_dims.items()}\n",
    "        buffer_shapes['g'] = (buffer_shapes['g'][0], self.dimg)\n",
    "        buffer_shapes['ag'] = (self.T, self.dimg)\n",
    "\n",
    "        buffer_size = (self.buffer_size // self.rollout_batch_size) * self.rollout_batch_size\n",
    "        self.buffer = ReplayBuffer(buffer_shapes, buffer_size, self.T, self.sample_transitions)\n",
    "\n",
    "    def random_action(self, n):\n",
    "        return torch.tensor(np.random.uniform(low=-1., high=1., size=(n, self.dimu)).astype(np.float32))\n",
    "\n",
    "    def get_actions(self, o, g, noise_eps=0., random_eps=0.):\n",
    "        actions = self.main.get_action(o, g)\n",
    "\n",
    "        noise = (noise_eps * np.random.randn(actions.shape[0], 4)).astype(np.float32)\n",
    "        actions += torch.tensor(noise).to(self.device)\n",
    "\n",
    "        actions = torch.clamp(actions, -1., 1.)\n",
    "        eps_greedy_noise = np.random.binomial(1, random_eps, actions.shape[0]).reshape(-1, 1)\n",
    "        random_action = self.random_action(actions.shape[0]).to(self.device)\n",
    "        actions += torch.tensor(eps_greedy_noise.astype(np.float32)).to(self.device) * (\n",
    "                    random_action - actions)  # eps-greedy\n",
    "        return actions\n",
    "\n",
    "    def store_episode(self, episode_batch):\n",
    "        \"\"\"\n",
    "        episode_batch: array of batch_size x (T or T+1) x dim_key\n",
    "                       'o' is of size T+1, others are of size T\n",
    "        \"\"\"\n",
    "        self.buffer.store_episode(episode_batch)\n",
    "\n",
    "        # add transitions to normalizer\n",
    "        episode_batch['o_2'] = episode_batch['o'][:, 1:, :]\n",
    "        episode_batch['ag_2'] = episode_batch['ag'][:, 1:, :]\n",
    "        shape = episode_batch['u'].shape\n",
    "        num_normalizing_transitions = shape[0] * shape[1]  # num_rollouts * (rollout_horizon - 1) --> total steps per cycle\n",
    "        transitions = self.sample_transitions(episode_batch, num_normalizing_transitions)\n",
    "\n",
    "        self.o_stats.update(transitions['o'])\n",
    "        self.o_stats.recompute_stats()\n",
    "\n",
    "        if self.use_goal:\n",
    "            self.g_stats.update(transitions['g'])\n",
    "            self.g_stats.recompute_stats()\n",
    "\n",
    "    def sample_batch(self):\n",
    "        transitions = self.buffer.sample(self.batch_size)\n",
    "        return [transitions[key] for key in self.stage_shapes.keys()]\n",
    "\n",
    "    def train(self):\n",
    "        batch = self.sample_batch()\n",
    "        batch_dict = OrderedDict([(key, batch[i].astype(np.float32).copy())\n",
    "                                 for i, key in enumerate(self.stage_shapes.keys())])\n",
    "        batch_dict['r'] = np.reshape(batch_dict['r'], [-1, 1])\n",
    "\n",
    "        main_batch = batch_dict\n",
    "        target_batch = batch_dict.copy()\n",
    "        target_batch['o'] = batch_dict['o_2']\n",
    "\n",
    "        self.main.compute_all(main_batch['o'], main_batch['g'],\n",
    "                              main_batch['u'])\n",
    "        self.target.compute_all(target_batch['o'], target_batch['g'],\n",
    "                                target_batch['u'])\n",
    "\n",
    "        # Q function loss\n",
    "        rewards = torch.tensor(main_batch['r'].astype(np.float32)).to(self.device)\n",
    "        discounted_reward = self.gamma * self.target.q_pi\n",
    "        target = torch.clamp(rewards + discounted_reward, -self.clip_return, 0.)\n",
    "        q_loss = torch.nn.MSELoss()(target.detach(), self.main.q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # policy loss\n",
    "        pi_loss = -self.main.q_pi.mean()\n",
    "        pi_loss += (self.main.pi ** 2).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        pi_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "    def update_target_net(self):\n",
    "        beta = 1. - self.polyak\n",
    "        for target, source in zip(self.target.parameters(), self.main.parameters()):\n",
    "            target.data.copy_(beta * source.data + self.polyak * target.data)\n",
    "\n",
    "    def create_network(self):\n",
    "        # for actor network\n",
    "        self.o_stats = Normalizer(size=self.dimo, eps=self.norm_eps, default_clip_range=self.norm_clip)\n",
    "        if self.use_goal:\n",
    "            self.g_stats = Normalizer(size=self.dimg, eps=self.norm_eps, default_clip_range=self.norm_clip)\n",
    "        else:\n",
    "            self.g_stats = None\n",
    "\n",
    "        self.main = ActorCritic(self.o_stats, self.g_stats, self.input_dims, self.use_goal).to(self.device)\n",
    "        self.target = ActorCritic(self.o_stats, self.g_stats, self.input_dims, self.use_goal).to(self.device)\n",
    "        self.target.actor = copy.deepcopy(self.main.actor)\n",
    "        self.target.critic = copy.deepcopy(self.main.critic)\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.main.actor.parameters(), lr=self.pi_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.main.critic.parameters(), lr=self.Q_lr)"
   ]
  }
 ]
}