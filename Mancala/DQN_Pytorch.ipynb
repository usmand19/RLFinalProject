{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0cbd785d33d8314b66158c604fc5ce4796ebc7b68ddf4a41ff3c1dd69f0fbd16d",
   "display_name": "Python 3.8.3 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import statistics\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "#from env import MancalaEnv\n",
    "from game import Game\n",
    "from randomagent import AgentRandom\n",
    "from exactagent import AgentExact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    #################################################\n",
    "    Initialize neural network model \n",
    "    Initialize parameters and build model.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ###################################################\n",
    "    Build a network that maps state -> action values.\n",
    "    \"\"\"\n",
    "    def forward(self, state):\n",
    "        \n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##################################################\n",
    "ReplayBuffer Class\n",
    "Defines  a Replay Memeory Buffer for a DQN or DDQN agent\n",
    "The buffer holds memories of: [sate, action reward, next sate, done] tuples\n",
    "Random batches of replay memories are sampled for learning. \n",
    "\"\"\"\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "            seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "##################################################\n",
    "Agent Class\n",
    "Defines DQN Agent Methods\n",
    "Agent interacts with and learns from an environment.\n",
    "\"\"\"\n",
    "class Agent():\n",
    "\n",
    "    \"\"\"\n",
    "    Initialize Agent, inclduing:\n",
    "        DQN Hyperparameters\n",
    "        Local and Targat State-Action Policy Networks\n",
    "        Replay Memory Buffer from Replay Buffer Class (define below)\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, dqn_type='DQN', replay_memory_size=1e5, batch_size=64, gamma=0.99,\n",
    "    \tlearning_rate=1e-3, target_tau=2e-3, update_rate=4, seed=0):\n",
    "        \n",
    "        \"\"\"\n",
    "        DQN Agent Parameters\n",
    "        ====== \n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            dqn_type (string): can be either 'DQN' for vanillia dqn learning (default) or 'DDQN' for double-DQN.\n",
    "            replay_memory size (int): size of the replay memory buffer (typically 5e4 to 5e6)\n",
    "            batch_size (int): size of the memory batch used for model updates (typically 32, 64 or 128)\n",
    "            gamma (float): paramete for setting the discoun ted value of future rewards (typically .95 to .995)\n",
    "            learning_rate (float): specifies the rate of model learing (typically 1e-4 to 1e-3))\n",
    "            seed (int): random seed for initializing training point.\n",
    "        \"\"\"\n",
    "        self.dqn_type = dqn_type\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.buffer_size = int(replay_memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.learn_rate = learning_rate\n",
    "        self.tau = target_tau\n",
    "        self.update_rate = update_rate\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        \"\"\"\n",
    "        # DQN Agent Q-Network\n",
    "        # For DQN training, two nerual network models are employed;\n",
    "        # (a) A network that is updated every (step % update_rate == 0)\n",
    "        # (b) A target network, with weights updated to equal the network at a slower (target_tau) rate.\n",
    "        # The slower modulation of the target network weights operates to stablize learning.\n",
    "        \"\"\"\n",
    "        self.network = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.target_network = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=self.learn_rate)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, self.buffer_size, self.batch_size, seed)\n",
    "\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "\n",
    "\n",
    "    ########################################################\n",
    "    # STEP() method\n",
    "    #\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_rate\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma)\n",
    "\n",
    "\n",
    "\t########################################################\n",
    "    # ACT() method\n",
    "    #\n",
    "    def act(self, state, eps=0.0):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.network(state)\n",
    "        self.network.train()\n",
    "\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "\t########################################################\n",
    "    # LEARN() method\n",
    "    # Update value parameters using given batch of experience tuples.\n",
    "    def learn(self, experiences, gamma, DQN=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get Q values from current observations (s, a) using model nextwork\n",
    "        Qsa = self.network(states).gather(1, actions)\n",
    "\n",
    "\n",
    "        if (self.dqn_type == 'DDQN'):\n",
    "        #Double DQN\n",
    "        #************************\n",
    "            Qsa_prime_actions = self.network(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "            Qsa_prime_targets = self.target_network(next_states)[Qsa_prime_actions].unsqueeze(1)\n",
    "\n",
    "        else:\n",
    "        #Regular (Vanilla) DQN\n",
    "        #************************\n",
    "            # Get max Q values for (s',a') from target model\n",
    "            Qsa_prime_target_values = self.target_network(next_states).detach()\n",
    "            Qsa_prime_targets = Qsa_prime_target_values.max(1)[0].unsqueeze(1)        \n",
    "\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        Qsa_targets = rewards + (gamma * Qsa_prime_targets * (1 - dones))\n",
    "        \n",
    "        # Compute loss (error)\n",
    "        loss = F.mse_loss(Qsa, Qsa_targets)\n",
    "\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.network, self.target_network, self.tau)\n",
    "\n",
    "\n",
    "    ########################################################\n",
    "    \"\"\"\n",
    "    Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    \"\"\"\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "###################################\n",
    "STEP 1: Set the Training Parameters\n",
    "======\n",
    "        num_episodes (int): maximum number of training episodes\n",
    "        epsilon (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        epsilon_min (float): minimum value of epsilon\n",
    "        epsilon_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "        scores (float): list to record the scores obtained from each episode\n",
    "        scores_average_window (int): the window size employed for calculating the average score (e.g. 100)\n",
    "        solved_score (float): the average score required for the environment to be considered solved\n",
    "        (here we set the solved_score a little higher than 13 [i.e., 14] to ensure robust learning).\n",
    "    \"\"\"\n",
    "num_episodes=2000\n",
    "epsilon=1.0\n",
    "epsilon_min=0.05\n",
    "epsilon_decay=0.99\n",
    "scores = []\n",
    "scores_average_window = 100      \n",
    "solved_score = 14 \n",
    "\n",
    "action_size = 6\n",
    "state_size = 14\n",
    "agent = Agent(state_size=state_size, action_size=action_size, dqn_type='DQN')\n",
    "opponent = AgentRandom()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4, 0, 5, 5, 5, 5, 0, 4, 4, 4, 4, 4, 4, 0]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "game = Game()\n",
    "score = game.move(1)\n",
    "print(game.board())\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "###################################\n",
    "STEP 6: Run the DQN Training Sequence\n",
    "The DQN RL Training Process involves the agent learning from repeated episodes of behaviour \n",
    "to map states to actions the maximize rewards received via environmental interaction.\n",
    "The artificial neural network is expected to converge on or approximate the optimal function \n",
    "that maps states to actions. \n",
    "The agent training process involves the following:\n",
    "(1) Reset the environment at the beginning of each episode.\n",
    "(2) Obtain (observe) current state, s, of the environment at time t\n",
    "(3) Use an epsilon-greedy policy to perform an action, a(t), in the environment \n",
    "    given s(t), where the greedy action policy is specified by the neural network.\n",
    "(4) Observe the result of the action in terms of the reward received and \n",
    "\tthe state of the environment at time t+1 (i.e., s(t+1))\n",
    "(5) Calculate the error between the actual and expected Q value for s(t),a(t),r(t) and s(t+1)\n",
    "\tto update the neural network weights.\n",
    "(6) Update episode score (total reward received) and set s(t) -> s(t+1).\n",
    "(7) If episode is done, break and repeat from (1), otherwise repeat from (3).\n",
    "Below we also exit the training process early if the environment is solved. \n",
    "That is, if the average score for the previous 100 episodes is greater than solved_score.\n",
    "\"\"\"\n",
    "\n",
    "# loop from num_episodes\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "\n",
    "    # reset the unity environment at the beginning of each episode\n",
    "    game = Game()    \n",
    "\n",
    "    # get initial state of the unity environment\n",
    "    state = game.board()\n",
    "\n",
    "    # set the initial episode score to zero.\n",
    "    score = 0\n",
    "\n",
    "    # Run the episode training loop;\n",
    "    # At each loop step take an epsilon-greedy action as a function of the current state observations\n",
    "    # Based on the resultant environmental state (next_state) and reward received update the Agent network\n",
    "    # If environment episode is done, exit loop...\n",
    "    # Otherwise repeat until done == true \n",
    "    while True:\n",
    "        # determine epsilon-greedy action from current sate\n",
    "        action = agent.act(state, epsilon)             \n",
    "\n",
    "        # send the action to the environment and receive resultant environment information\n",
    "        score = game.move(action)\n",
    "        #env_info = env.step(action)[brain_name]        \n",
    "\n",
    "        #next_state = env_info.vector_observations[0]   # get the next state\n",
    "        next_state = game.board()\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "\n",
    "        #Send (S, A, R, S') info to the DQN agent for a neural network update\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "\n",
    "        # set new state to current state for determining next action\n",
    "        state = next_state\n",
    "\n",
    "        # Update episode score\n",
    "        score += reward\n",
    "\n",
    "        # If unity indicates that episode is done, \n",
    "        # then exit episode loop, to begin new episode\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Add episode score to Scores and...\n",
    "    # Calculate mean score over last 100 episodes \n",
    "    # Mean score is calculated over current episodes until i_episode > 100\n",
    "    scores.append(score)\n",
    "    average_score = np.mean(scores[i_episode-min(i_episode,scores_average_window):i_episode+1])\n",
    "\n",
    "    # Decrease epsilon for epsilon-greedy policy by decay rate\n",
    "    # Use max method to make sure epsilon doesn't decrease below epsilon_min\n",
    "    epsilon = max(epsilon_min, epsilon_decay*epsilon)\n",
    "\n",
    "    # (Over-) Print current average score\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score), end=\"\")\n",
    "\n",
    "    # Print average score every scores_average_window episodes\n",
    "    if i_episode % scores_average_window == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
    "    \n",
    "    # Check to see if the task is solved (i.e,. avearge_score > solved_score). \n",
    "    # If yes, save the network weights and scores and end training.\n",
    "    if average_score >= solved_score:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
    "\n",
    "        # Save trained neural network weights\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        nn_filename = \"dqnAgent_Trained_Model_\" + timestr + \".pth\"\n",
    "        torch.save(agent.network.state_dict(), nn_filename)\n",
    "\n",
    "        # Save the recorded Scores data\n",
    "        scores_filename = \"dqnAgent_scores_\" + timestr + \".csv\"\n",
    "        np.savetxt(scores_filename, scores, delimiter=\",\")\n",
    "        break"
   ]
  }
 ]
}